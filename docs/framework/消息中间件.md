# 消息中间件

## 目录

- [放大100倍压力，找出你系统的技术挑战](#放大100倍压力，找出你系统的技术挑战)
- [消息中间件](#消息中间件)
    - [消息中间件作用](#消息中间件作用)
    - [Kafka、RabbitMQ 以及 RocketMQ对比](#Kafka、RabbitMQ以及RocketMQ对比)
- [RocketMQ](#RocketMQ)
    - [RocketMQ架构原理](#RocketMQ架构原理)
        - [NameServer](#NameServer)
        - [Broker主从架构](#Broker主从架构)
        - [RocketMQ的核心数据模型](#RocketMQ的核心数据模型) 
        - [RocketMQ的集群部署](#RocketMQ的集群部署)
          - [小规模RocketMQ的集群部署](#小规模RocketMQ的集群部署)
          - [RocketMQ的集群可视化监控](#RocketMQ的集群可视化监控)
          - [RocketMQ生产参数调整](#RocketMQ生产参数调整)
          - [RocketMQ的集群压测](#RocketMQ的集群压测)
          - [RocketMQ生产集群的规划](#RocketMQ生产集群的规划)
        - [RocketMQ生产者发送消息模式](#RocketMQ生产者发送消息模式)
        - [RocketMQ的消费模式](#RocketMQ的消费模式)
        - [RocketMQ的消费模式](#RocketMQ的消费模式)
  - [RocketMQ底层原理](#RocketMQ底层原理)
        - [生产者往Broker集群发送消息的底层逻辑](#生产者往Broker集群发送消息的底层逻辑)
        - [Broker接受消息后如何在存储到磁盘上的](#Broker接受消息后如何在存储到磁盘上的)
        - [基于Dledger技术部署Broker高可用集群，到底如何进行数据同步](#基于Dledger技术部署Broker高可用集群，到底如何进行数据同步)
        - [消费者到底是基于什么策略选择Master或Slave拉取数据](#消费者到底是基于什么策略选择Master或Slave拉取数据)
        - [消费者是如何从Broker拉取消息进行处理以及ACK的？如果消费者故障会如何处理](#消费者是如何从Broker拉取消息进行处理以及ACK的？如果消费者故障会如何处理)
        - [RocketMQ 是如何基于Netty扩展出高性能网络通信架构的？](#RocketMQ是如何基于Netty扩展出高性能网络通信架构的)


# 消息中间件

## 放大100倍压力，找出你系统的技术挑战

思考一：系统的核心链路，有哪些步骤，各个步骤的性能如何，是否有改进空间？
    
        下单--预约--核销---退款
         其余的流程可以走MQ异步处理
思考二：系统中是否有类似后台线程定时补偿的逻辑
    
    订单长时间未支付，要关闭，
    预约单长时间无人确认接单，自动分单，
    秒杀活动/优惠券设置时间后要自动开始，到点后要自动结束

思考三：系统中有哪些和第三方系统的耦合？

        耦合微信支付，短信，推送等
          
        考虑MQ解耦

思考四：核心链路中是否存在哪些关键步骤可能会失败的情况？万一失败了该怎么办？

     例如退款失败后怎么办

思考五：平时是否有其他系统需要获取你们的数据的情况？他们是如何获取数据的？

    是直接跑SQL从你们的数据库里查询？或者是调用你们的接口来获取数据？
    是否有这种情况？如果有，对你们有什么影响吗？
  
    可以基于mysql的binlog 日志将数据发送给MQ，别的部门从MQ中消费消息

思考六：你们的系统是否存在流量洪峰的情况，有时候突然之间访问量增大好几倍，是否对你们的系统产生无法承受的压力？
        
## 消息中间件

### 消息中间件作用

解耦
异步
消峰

### Kafka、RabbitMQ以及RocketMQ对比

（1）、Kafka的优势与劣势

   kafka 性能很高，基本发消息给kafka都是毫秒级的性能，可用性也很高，kafka支持集群部署的，其中部分宕机是可以继续运行的

   但是kafka比较为人诟病的一点，似乎是丢数据方能的问题，因为kafka收到消息后会写入一个磁盘缓冲区里，并没有落地到物理磁盘上，
    
   所以要是机器本身故障，可能会导致磁盘缓冲区数据丢失。
   
   而且kafka另外一个比较大的缺点，就是功能非常单一，主要支持发送消息给他，然后从里面消费消息，其他的就没有什么额外的高级功能了
   
   因此，综上所述，基本行业里的标准，是把kafka在用户行为日志的采集和传输上的，比如大数据团队要收集app上用于的一些行为日志，这种日志就是
   
   kafka收集传输的。

（2）、RabbitMQ的优势与劣势

   RabbitMQ的优势是可以保证数据不丢失，也可以保证高可用性，即集群部署的时候。部分机器宕机可以继续运行，然后支持部分高级功能
   比如说：死信队列，消息重试之类的

   但是他有个缺点最为人诟病的，就是RabbitMQ吞吐量比较低，一般就是每秒几万级别的，所以遇到特别高特别高的并发的情况下，支撑起来是有点困难的

   还有一个是它进行集群扩展的时候(就是加机器部署)，还是比较麻烦的


（2）、RocketMQ的优势与劣势

   RocketMQ吞吐量也同样很高，单机可以达到10wQPS以上，而且可以保证高可用，性能很高，而且支持配置，保证数据绝对不丢失，可以部署
   大规模集群，而且支持各种高级性能，比如说：延迟消息，事务消息，消息回溯，死信队列，消息积压等等
   
   
## RocketMQ

### RocketMQ架构原理

![img_1.png](images/RockerMQjiagou.png)

RocketMQ 这个技术一共包含了四个核心部分

    1.第一块就是他的NameServer，这个东西很重要，他要负责管理集群里所有Broker的信息，让使用MQ的系统能感知到集群里有哪些Broke。
    2.第二块就是Broke集群的本身信息，必须在多台机器上部署这么一个集群，而且还得用主从架构实现数据多副本和高可用。
    3.第三块就是消息生产者
    4.第四块就是消息的消费者

#### NameServer

NameServer的设计是采用Peer-to-Peer的模式来做的，可以集群化部署，但是里面任何一台机器都是独立运行的，跟其他机器没有任何通信。

要部署RocketMQ，就得先部署NameServer，NameServer支持集群化部署，做到高可用。任何一台机器宕机，NameServer可以继续对外提供服务。

每个Broker启动时都得向所有的NameServer进行注册，也就是说，每个NameServer都有一份集群中所有的Broker信息。

RocketMQ中的生产者和消费者自己主动区NameServer拉取Broker信息的

Broker和NameServer之间通过心跳机制（采用的是TCP长连接），Broker会每隔30s给所有的NameServer发送心跳，告诉NameServer自己还活着。

每次NameServer收到Broker心跳，就可以更新一下它的最近一次心跳时间。

然后NameServer 会每隔10s运行一个任务，区检查各个Broker的最近一次心跳，如果某个Broker超过120s都没发送心跳，那么就认为这个Broker已经挂掉。

生产者和消费者会重新获取NameServer最新的路由信息，并缓存在本地

#### Broker主从架构

为了保证RocketMQ的数据不丢失而且具备一定的高可用性，所以一般将Broker，部署成Master-Slave模式的，也就时一个Master Broker  对应一个Slave Broker

RocketMQ的Master-Slave模式采取的是（Pull模式）Slave Broker不停的发送请求到Master Broker 去拉取消息

**写入数据**的时候，肯定是选择**Master Broker** 去写入的

**读取数据**的时候，**有可能是Master Broker获取，有可能是Slave Broker 获取**，一切根据当时的情况来定。

如果Slave Broker 挂掉了，那么读写的压力都集中在Master Broker上。

如果Master Broker 挂掉了，在RocketMQ4.5版本之前，Slave Broker无法自定切换为Master Broker,会导致服务不可用。

RocketMQ 4.5之后的版本，RocketMQ支持一种新的机制，叫Dledger，可实现RocketMQ高可用自动切换的效果。

(备注:redis 的Master-Slaver 是基于哨兵模式，异曲同工）



#### RocketMQ的核心数据模型

MQ的核心数据模型 Topic

Topic其实就是一个（逻辑上）数据集合的意思，不同的数据，你的放到不同的Topic里面。

每个Topic分布式存储在Master Broker上，Slave Broker或同步Master Broker 数据。

生产者跟NameServer建立一个TCP长连接，然后定时的从他那里拉取最新的路由信息，包含集群中有哪些Broker，哪些Topic，每个Topic存储在哪些Broker 上

然后生产者找到自己要投递的Topic分布在哪些Broker上，根据负载均衡算法，选择出一台Broker出来，然后跟这个Broker页建立一个TCP长连接，然后通过长连接向Broker发送消息即可。

这里要注意的是生产者一定是投递消息到Master Broker的，然后通过Master Broker会同步数据到他的Slave Broker上

实现一份数据多个副本，保证Master Broker故障之后，数据不丢失，而且可以把Slave Broker切换为Master Broker提供服务。


消费者页生产者类似，他们也会从NameServer建立长连接，然后拉取路由消息，找到自己消息的Topic在哪几台Broker上，就可以跟Broker建立长连接，从里面拉取消息了。

这里唯一要注意的就是消费者系统可能会从Master Broker拉取消息，也可以从Slave Broker拉取消息，都有可能，一切看具体情况。


#### RocketMQ的集群部署

##### 小规模RocketMQ的集群部署

##### RocketMQ的集群可视化监控

##### RocketMQ生产参数调整

##### RocketMQ的集群压测

##### RocketMQ生产集群的规划

### RocketMQ生产者发送消息模式

    同步发送：生产者发送消息出去之后，登台MQ返回通知，程序在接着向下执行
    异步发送：生产者发送消息出去，无需登台MQ返回，直接向下执行，待MQ响应之后，callBack函数，
            如果发送成功，则调用onSuccess函数
            如果发送失败，则调用onException函数
    单向发送：生产者发送消息出去之后，代码向下执行，不关注MQ是否返回结果，无论消息发送成功或者发送失败都不管你的事。

### RocketMQ的消费模式

Push消费模式：就是Broker主动把消息发送给你的消费者
Pull消费模式：就是消费者主动发送请求到Broker去拉取消息

## RocketMQ底层原理

### 生产者往Broker集群发送消息的底层逻辑


topic数据分片机制：

![img_1.png](images/topicfenpianjizhi.png)

    Message Queue 将一个Topic的数据拆分成了很多个数据分片，然后再每个Broker机器上存储一些Message Queue

生产者写入数据的过程：

![img.png](images/sehngchanzhexierushuju.png)

Broker故障时：

Master Broker故障时，此时正在等待其他Slave Broker自动热切换为Master Broker，但是在这个过程中，这一组Broker是没有Master Broker可以写入的。

通常建议大家打开 Product 中开启一个开关，就是sendLatencyFaultEnable

一旦打开这个开关，那么他就会有一个自动容错机制，比如某次发现一个Broker无法访问，则自动回避访问这个Broker一段时间，过段时间再去访问他。

### Broker接受消息后如何在存储到磁盘上的

    Broker 数据存储实际才是MQ最核心的环节，他决定了生产者消息写入的吞吐量，决定了消息不能丢失，决定了消费者获取消息的吞吐量，这些都是由他决定的。

![img.png](images/Brokercunchujizhi.png)

    首先，当生产者的消息发送到Broker上的时候，他会把这个消息写入到磁盘上的日志文件，叫做commitLog，直接顺序写入这个文件，（先进入OS Cache Page ，再刷入磁盘）
    这个CommitLog是很多个磁盘文件，每个文件限定差不多1GB
    Broker收到消息之后就直接追加写入这个文件的末尾，如果一个CommitLog写满了1GB，就会重新创建一个CommitLog
    
    在Broker中，对Topic下的每个MessageQueue都会有一系列的ConsumeQueue文件
   
    格式大致为 $HOME/store/consumeQueue/{topic}/{queueId}/{consumeQueueFileName}
    topic：就是指逻辑上的那个Topic
    queueId：值某个那个Topic下的某个MessageQueue
    consumeQueueFileName：MessageQueue对应的consumeQueue文件名称

    当你的Broker收到消息写入到CommitLog之后，其实他同时会将这条数据在CommitLog中的物理位置，也就是一个文件的偏移量（offset），写入到这条消息所属的
    MessageQueue对应的consumeQueue文件中去（offset其实的是CommitLog文件消息的地址引用）。
   
    实际上consumeQueue中存储的每条数据不止消息在CommitLog中的offset偏移量，还包含了消息的长度，以及tag，hashCode，一条数据是20个字节，每个consumeQueue保存30W条数据
    大概每个文件是5.72M

    所以实际上Topic的每个MessageQueue都对应了Broker机器上的多个ConsumeQueue文件，这些文件保存了这个MessageQueue的所有消息在CommitLog文件中的物理位置，也就是offset偏移量。

为了提升CommitLog文件的写入性能

    先写OS操作系统的Page Cache 和顺序写两个机制来提升写入CommitLog文件的性能

同步刷盘和异步刷盘

    异步刷盘模式下，写入到OS Page Cache 缓存成功后，直接提交ACK给生产者，如果刷盘失败，会导致数据丢失，但是吞吐量很高

    同步刷盘模式下，写入到OS Page Cache 缓存成功后，必须强制把这台哦消息刷入到底层的物理磁盘，然后才返回ACK给生产者，此时你才知道消息写入成功，
    这个模式下，保证Master 数据不会丢失，但是吞吐量下降。
    返回ACK后，如果Master挂掉，数据没同步，切换到Slave时，也会出现数据丢失。


### 基于Dledger技术部署Broker高可用集群，到底如何进行数据同步

![img.png](images/Dledger.png)

Dledger技术实际上首先他自己就有一个CommitLog机制，你把数据交给他，他会写CommitLog磁盘文件里去，这是他能干的第一件事情。

如果以及Dledger技术来实现Broker高可用架构，实际上就是用Dledger先替换掉原来的Broker，Dledger自己来来管理CommitLog

那么就是每个Broker上都有一个Dledger组件，

Dledger是基于Raft协议来进行Leader Broker选举的，会从中选举出一个Leader来

Raft协议投票原理：
    
        比如三台机器，第一轮投票开始，他们都把票投给自己，结果每个人得到的票数一样，第一轮选举失败

        接着每个人进入一个随机休眠，如果是第一个人休息三秒，第二个人休息5秒，四但个人休息4秒
    
        接着第一个人开始醒过来，继续把票投给自己，然后发送自己的选票给别人，下一个人醒来时，发现自己没有票
    
        会把票投给那个有票的人，接着第三个人醒来同理

        依靠这个休眠机制一般都能选出一个Leader

Dledger是基于Raft协议进行多副本同步

Raft协议多副本同步机制：

        首先Leader Broker上的Dledger收到一条数据之后，会标记uncommitted 状态
        然后他会通过自己的DledgerServer组件把这个uncommitted数据发送给Following Broker的Dledger
        接着Follower Broker的DledgerServer收到uncommitted消息之后，必须返回一个ack给Leader Broker的DledgerServer
        然后如果Leader Broker收到超过半数的Follower Broker返回的ack之后，会将消息标记为committed状态
        然后Leader Broker上的DledgerServer就会发送commited消息给Follower Broker机器的DledgerServer，让他们也把消息标记为committed状态
        
如果Leader Broker发生宕机了
    
如果Leader Broker 发生宕机了，剩下的两个Follower Broker会重新发起选举，他们还是会采用Raft协议的算法，去选举出来一个新的Leader Broker继续对外提供服务，
而且会对没有完成数据同步的进行一些恢复性操作，保证数据不会丢失。


### 消费者到底是基于什么策略选择Master或Slave拉取数据

集群模式：一个消费者组获取一条消息，只会交给组内的一台机器取处理，而不是每台机器都可以获取到这条消息的
广播模式：那么对于消费者组获取到一条消息，组内的每台机器都可以获取到这条消，但是相对而言，广播模式用的很少，常见基本都是使用集群消费模式
一个Topic的多个MessageQueue会均匀分摊给消费者组内的多个机器取消费，这里的一个原则就是，
一个MessageQueue只能被一个消费机器取消费，但是一个消费机器，可以负责多个MessageQueue的消息处理

Push模式和Pull模式
实际上这两个模式是一样的，都是消费者机器主动发送请求到Broker机器上取拉取一批消息下来。

Push消费模式本质底层也是基于这种消费者主动拉取的模式实现的，只不过他的名字叫Push而已，意思是Broker会尽可能的实时的把消息
交给消费者机器来处理，他的消息实时性会更好。
一般我们使用RocketMQ的时候，消费者模式通常都是基于Push模式来做的，因为Pull模式的代码写起来更加复杂和繁琐，
而且Push模式底层本身就是基于消息拉取来做的，只不过实时性更好。

push模式实现思路：当消费者发送请求到Broker取拉取消息的时候，如果有新消息可以消费那么立马就会返回一批消息到消费机器取处理。
处理完之后会接着立刻发送请求到Broker机器去拉取下一批消息。

所以消费机器再push模式下会处理完一批消息，立马发送请求去拉取下一批消息，消息处理的实时性非常好，看起来就跟Broker
一直不停的推送消息到消费者机器一样。

push模式下有一个请求挂起和长轮询的机制

当你的请求发送到Broker，结果他发现没有新的消息给你处理的时候，就会请求线程挂起，默认挂起15s，然后这个期间
他会有后台线程每隔一会儿就会检查一下是否有新的消息给你，另外如果再这个挂起过程中，如果有新的消息到达了就会主动唤起挂起的线程
，然后把消息返回给你。

其实消费消息的时候，本质就是根据你要的消费的MessageQueue以及开始消费的位置，去找对应的ConsumeQueue读取里面对应位置的消息
再commitLog中的物理offset偏移量，然后到CommitLog中更具offset读取消息数据返回给消费者机器。

ConsumeQueue 文件同样也是基于 os page cache 来进行优化的

也就是说，对于Broker机器磁盘上的大量的ConsumeQueue文件，再写入的时候也会优先进入os cache,

而且os 自己有一个优化机制，就是读取一个磁盘文件的时候，他就会把磁盘文件的一些数据缓存到os cache中。

CommitLog 是基于os cache + 磁盘一起读取的

当你去拉取消息的时候，可以轻松从os cache 里读取少量的consumeQueue文件里的offset，这个性能是极高的，
但是当你去committedLog文件里去读取完整消息的时候，会有两种可能

第一种可能：如果你读取的是那种刚刚写入commitLog的数据，那么大概率还停留再 os cache,
此时你可以顺利的直接从 os cache 中读取commitLog中的数据
第二种可能：你读取的是比较早之前写入的commitLog的数据，哪些数据很早就被刷入磁盘了，已经不在os cache 里了，
那么此时你读取就只能从磁盘文件里读取了


### 消费者是如何从Broker拉取消息进行处理以及ACK的？如果消费者故障会如何处理

当消费者处理完这批消息之后，消费者机器就会提交我们的目前一个消费进度到Broker上去，然后Broker就会存储我们的消费进度

下次这个消费者组只要再次拉取这个ConsumeQueue的消息，就可以从Broker记录的消费位置开始拉取，不用从头开始拉取了。

如果消费者族中机器宕机或扩展机器，这个时候其实会进入一个rebalance的环节，也就是说重新给各个消费者机器分配他们要处理的MessageQueue

### RocketMQ是如何基于Netty扩展出高性能网络通信架构的

![img.png](images/Reactormoxing.png)
首先作为Broker而言，他会有一个Reactor主线程，而且这个线程负责监听一个网络端口

假设我么有一个Producer他现在想跟Broker建立TCP长连接，此时Broker上的这个Reactor主线程，他会在端口上监听到这个Producer建立连接的请求

接着这个Reactor主线程就专门负责这个Producer按照TCP协议规定的一系列步骤和规范，建立好一个长连接

Producer里面会有一个SocketChannel，Broker里也会有一个SocketChannel，这两个SocketChannel就代表他们两建立好这个长连接

接着Producer会通过这个SocketChannel去发送消息给Broker。

Reactor线程池，默认是3个线程

Reactor主线程建立好每个连接SocketChannel，都会交给这个Reactor线程池里的其中一个线程去监听请求

基于Worker线程池完成一系列准备工作

线程池会转交请求给Worker线程池，进行一系列预处理 

Worker线程池默认有8个线程，此时Reactor线程收到的这个请求会交给Worker线程池中的一个线程进行处理，完成ssl加密，编码解码，连接空闲检查，网络连接管理诸如此类的事情。


预处理之后，请求转交给业务线程池

比如对于处理发送请求而言，就会把请求转发给SendMessage线程池。SendMessage是可以配置的配置的越高，处理消息的吞吐量越高


